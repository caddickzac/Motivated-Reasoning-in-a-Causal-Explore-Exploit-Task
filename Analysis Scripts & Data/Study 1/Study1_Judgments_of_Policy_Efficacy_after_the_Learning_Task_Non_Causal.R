########################################################
# Judgments of Policy Efficacy after the Learning Task #
########################################################

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#################################
# Analysis: No Effect Functions #
#################################

# Dataframe:
SA_F5_F6

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#####################
# Judgment Accuracy #
#####################

# Question: Do participants acccurately assess no effect functions?
# Lower scores are more accurate (0 = perfect accuracy, 
#                               5 = maximally wrong asessment








#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



#====================================


Study1_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal <- 
  select(SA_F5_F6, User, F5_Final.Slider.Acc.ABS, F6_Final.Slider.Acc.ABS,
         F5_Final.Slider.Acc.Bias, F6_Final.Slider.Acc.Bias, 
         F5_F6_Slider_Acc_Bias)
names(Study1_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal) <-
  c("Participant", "F5_Raw_Judgment_Value", "F6_Raw_Judgment_Value",
    "F5_Scored_Judgment_Bias", "F6_Scored_Judgment_Bias",
    "Average_Judgment_Bias")

write.csv(Study1_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal,
          "Study1_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal.csv")

########################################################################
# github code below
########################################################################

###########################################
# Judgment Accuracy: Non-Causal Functions #
###########################################

#import libraries
library('rcompanion') # 'wilcoxonOneSampleR()' 
library("tidyverse")

# Import data
Study1_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal <- read.csv("Study1_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal.csv")

# Preview data
head(Study1_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal)

# data dictionary:
## "Participant" = participant number
## "F5_Raw_Judgment_Value" = raw slider value for function 5 final judgment (0-10 scale; 0=policy A is best; 5=no effect; 10 = policy B is best)
## "F6_Raw_Judgment_Value" = raw slider value for function 6 final judgment (0-10 scale; 0=policy A is best; 5=no effect; 10 = policy B is best)
## "F5_Scored_Judgment_Bias" = coded version of final judgment for function 5. Lower scores are more biased 
###   Calculated by taking the absolute of the preferred outcome value (either 0 or 10) subtracted by raw judgment value 
###   possible values: 0 = no distance from preferred policy, 5 = correct assessment; 10 = maximal distance from preferred policy.
## "F6_Scored_Judgment_Bias" = coded version of final judgment for function 6. Lower scores are more biased 
###   Calculated by taking the absolute of the preferred outcome value (either 0 or 10) subtracted by raw judgment value 
###   possible values: 0 = no distance from preferred policy, 5 = correct assessment; 10 = maximal distance from preferred policy.                      
## "Average_Judgment_Bias" = this value is the average of "F5_Scored_Judgment_Bias" & "F6_Scored_Judgment_Bias". This represents participants average bias for the two non-causal functions (F5/F6)

# descriptives
summary(Study1_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Average_Judgment_Bias) # median = 4

wilcox.test(Study1_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Average_Judgment_Bias, mu = 5) # aggregated at user level (averaged Functions F5+F6)
#Note: that mu is set to 5 because that is mid point of scale, where 5 represents no-effect/no-preference. 

# Effect size
wilcoxonOneSampleR(Study1_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Average_Judgment_Bias, 
                   mu=5)

# One-sample Wilcoxon signed-rank test
# W = 42.50, p < .001, r = .651

