###########################################
# Judgment Accuracy: Non-Causal Functions #
###########################################

#import libraries
library('lmerTest') 
library("tidyverse")
library('boot') # used for "inv.logit()" function

# Import data
Study2B_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal <- as_tibble(read.csv("Study2B_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal.csv"))

# Preview data
head(Study2B_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal)

# data dictionary:
## "Participant" = participant number
## "Function" = function number (non-causal functions only; range 5-6)
## "Strong_Preference" = denotes if participant had a strong preference for one of the policies (or not)
## "Judgment_Accuracy" = values are the distance a final judgment was from the correct assessment value (2 possible values: 0-1); 0=correct judgment; 1=incorrect (e.g., Policy A < Policy B)
## "Judgment_Bias" = values are distance from preferred policy judgment; 0=chose preferred policy as being best; 1=choose (correctly) that Policy A = Policy B; 2=choose non-preferred policy as being better

# Confidence Interval Function (used for computing confidence intervals from regression models)
CI_function <- function(model){
  m <- model
  se <- sqrt(diag(vcov(m)))
  tab <- cbind(Est = fixef(m), LL = fixef(m) - 2 * se, UL = fixef(m) + 2 * se)
  inv_tab <- inv.logit(tab) # probability scale
  labels <- c("Mean", "LowerBound", "UpperBound")
  values <- c(inv_tab[1], inv_tab[2], inv_tab[3])
  output <- data_frame(labels, values)
  output$labels <- as.factor(output$labels)
  return(output)
}

######################################
# Non-Causal: Judgment Bias Analysis #
######################################

jpe_non_causal_strong_preference <- Study2B_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal %>% 
  filter(Strong_Preference=="Yes") %>%  # omit neutral policies
  filter(Judgment_Bias!=1) # omit those who answered correctly

# Re-code so that 1 = choose preferred policy; 0 = choose non-preferred policy
jpe_non_causal_strong_preference$Judgment_Bias_Coded <-
  ifelse(jpe_non_causal_strong_preference$Judgment_Bias==0,1,0)


# Judgment Bias (only including people who did not correctly assess)
m1 <- glmer(Judgment_Bias_Coded ~ (1|Participant), 
            data=jpe_non_causal_strong_preference, family='binomial')
summary(m1)
# Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
# Family: binomial  ( logit )
# Formula: Judgment_Bias_Coded ~ (1 | Participant)
# Data: jpe_non_causal_strong_preference
# 
# AIC      BIC   logLik deviance df.resid 
# 293.6    300.6   -144.8    289.6      241 
# 
# Scaled residuals: 
#   Min      1Q  Median      3Q     Max 
# -1.5423 -1.3442  0.5846  0.5846  0.6484 
# 
# Random effects:
#   Groups      Name        Variance Std.Dev.
# Participant (Intercept) 0.3127   0.5592  
# Number of obs: 243, groups:  Participant, 197
# 
# Fixed effects:
#   Estimate Std. Error z value Pr(>|z|)    
# (Intercept)   0.9942     0.1982   5.016 5.29e-07 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

# get confidence interval from model
CI_function(m1)

#-----------------------------------------------------------------------------

######################################
# Strong vs Weak Preference Analysis #
######################################

# Effects coding for model
Study2B_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Strong_Preference2 <-
  ifelse(Study2B_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Strong_Preference=="No",.5,-.5)

# Re-code judgment accuracy so that 1=correct judgment; 0=incorrect judgment
Study2B_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Judgment_Accuracy_Coded <- 
  ifelse(Study2B_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Judgment_Accuracy==0,1,0)

# Descriptives
tapply(Study2B_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Judgment_Accuracy_Coded,
       list(Study2B_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Strong_Preference),
       mean)

# Modeling
# m1 <- glmer(as.factor(Judgment_Accuracy_Coded) ~ Strong_Preference2 + (1+Strong_Preference2|Participant),
#             data=Study2B_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal, family="binomial",
#             control=glmerControl(optimizer="bobyqa"))
# convergence issues

m2 <- glmer(as.factor(Judgment_Accuracy_Coded) ~ Strong_Preference2 + (1+Strong_Preference2||Participant),
            data=Study2B_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal, family="binomial",
            control=glmerControl(optimizer="bobyqa"))
summary(m2)

# Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
# Family: binomial  ( logit )
# Formula: as.factor(Judgment_Accuracy_Coded) ~ Strong_Preference2 + (1 +      Strong_Preference2 || Participant)
# Data: Study2B_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal
# Control: glmerControl(optimizer = "bobyqa")
# 
# AIC      BIC   logLik deviance df.resid 
# 468.6    486.0   -230.3    460.6      562 
# 
# Scaled residuals: 
#   Min       1Q   Median       3Q      Max 
# -0.97273 -0.01256 -0.00772 -0.00766  1.03131 
# 
# Random effects:
#   Groups        Name               Variance Std.Dev.
# Participant   (Intercept)        164.8    12.84   
# Participant.1 Strong_Preference2 592.4    24.34   
# Number of obs: 566, groups:  Participant, 283
# 
# Fixed effects:
#   Estimate Std. Error z value Pr(>|z|)    
# (Intercept)         -9.2050     0.7168 -12.842   <2e-16 ***
#   Strong_Preference2   1.0014     0.8605   1.164    0.245    
# ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

# get confidence interval from model
CI_function(m2)
