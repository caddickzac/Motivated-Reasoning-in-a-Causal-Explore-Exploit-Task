###########################################
# Judgment Accuracy: Non-Causal Functions #
###########################################

# import libraries
library('lmerTest') 
library("tidyverse")
library('boot') # used for "inv.logit()" function

# Import data
Study3_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal <- as_tibble(read.csv("Study3_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal.csv"))

# Preview data
head(Study3_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal)

# data dictionary:
## "Participant" = participant number
## "Function_Exposure_Treatment" = denotes whether or not participant received function exposure treatment (see Study 3 method section for details;), 0=no, 1=yes
## "Function" = function number (non-causal functions only; range 5-6)
## "Strong_Preference" = denotes if participant had a strong preference for one of the policies (or not)
## "Judgment_Accuracy" = values are the distance a final judgment was from the correct assessment value (2 possible values: 0-1); 0=correct judgment; 1=incorrect (e.g., Policy A < Policy B)
## "Judgment_Bias" = values are distance from preferred policy judgment; 0=chose preferred policy as being best; 1=choose (correctly) that Policy A = Policy B; 2=choose non-preferred policy as being better

# Confidence Interval Function (used for computing confidence intervals from regression models)
CI_function <- function(model){
  m <- model
  se <- sqrt(diag(vcov(m)))
  tab <- cbind(Est = fixef(m), LL = fixef(m) - 2 * se, UL = fixef(m) + 2 * se)
  inv_tab <- inv.logit(tab) # probability scale
  labels <- c("Mean", "LowerBound", "UpperBound")
  values <- c(inv_tab[1], inv_tab[2], inv_tab[3])
  output <- tibble(labels, values)
  output$labels <- as.factor(output$labels)
  return(output)
}

######################################
# Non-Causal: Judgment Bias Analysis #
######################################

jpe_non_causal_replication <- Study3_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal %>% 
  filter(Judgment_Bias!=1) %>%# omit those who answered correctly
  filter(Function_Exposure_Treatment==0) # omit participants who were given "Function_Exposure_Treatment"

# Re-code so that 1 = choose preferred policy; 0 = choose non-preferred policy
jpe_non_causal_replication$Judgment_Bias_Coded <-
  ifelse(jpe_non_causal_replication$Judgment_Bias==0,1,0)


# Judgment Bias (only including people who did not correctly assess)
m1 <- glmer(Judgment_Bias_Coded ~ (1|Participant), 
            data=jpe_non_causal_replication, family='binomial')
summary(m1)

# Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
# Family: binomial  ( logit )
# Formula: Judgment_Bias_Coded ~ (1 | Participant)
# Data: jpe_non_causal_replication
# 
# AIC      BIC   logLik deviance df.resid 
# 77.5     81.8    -36.7     73.5       61 
# 
# Scaled residuals: 
#   Min      1Q  Median      3Q     Max 
# -1.6450 -1.6450  0.6079  0.6079  0.6079 
# 
# Random effects:
#   Groups      Name        Variance Std.Dev.
# Participant (Intercept) 0        0       
# Number of obs: 63, groups:  Participant, 38
# 
# Fixed effects:
#   Estimate Std. Error z value Pr(>|z|)    
# (Intercept)   0.9954     0.2838   3.507 0.000453 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

# confidence interval from model
CI_function(m1)

#-----------------------------------------------------------------------------------

#######################################################
# Function Exposure vs No Function Exposure Treatment #
#######################################################

Study3_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Judgment_Accuracy_Coded <- 
  ifelse(Study3_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Judgment_Accuracy==0,1,0)


# Descriptives
aggregate(Study3_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Judgment_Accuracy_Coded,
       by=list(Study3_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Function_Exposure_Treatment),
       mean) %>% 
  rename("Function_Exposure_Treatment" = "Group.1",
         "mean"= "x") # 0 = no function exposure; 1= function exposure condition

# effects coding
Study3_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Function_Exposure_Treatment2 <-
  ifelse(Study3_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Function_Exposure_Treatment==1,.5,-.5)


# Judgment Bias (only including people who did not correctly assess)
m1 <- glmer(Judgment_Accuracy_Coded ~ Function_Exposure_Treatment2 + (1|Participant), 
            data=Study3_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal, family='binomial')
summary(m1)
# Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
# Family: binomial  ( logit )
# Formula: Judgment_Accuracy_Coded ~ Function_Exposure_Treatment2 + (1 |      Participant)
# Data: Study3_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal
# 
# AIC      BIC   logLik deviance df.resid 
# 166.9    176.0    -80.5    160.9      148 
# 
# Scaled residuals: 
#   Min      1Q  Median      3Q     Max 
# -0.6310 -0.4035 -0.3814 -0.3814  1.6635 
# 
# Random effects:
#   Groups      Name        Variance Std.Dev.
# Participant (Intercept) 1.258    1.122   
# Number of obs: 151, groups:  Participant, 77
# 
# Fixed effects:
#   Estimate Std. Error z value Pr(>|z|)    
# (Intercept)                   -1.5354     0.3699  -4.151 3.31e-05 ***
#   Function_Exposure_Treatment2  -0.1454     0.5082  -0.286    0.775    
# ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# Correlation of Fixed Effects:
#   (Intr)
# Fnctn_Ex_T2 0.120
