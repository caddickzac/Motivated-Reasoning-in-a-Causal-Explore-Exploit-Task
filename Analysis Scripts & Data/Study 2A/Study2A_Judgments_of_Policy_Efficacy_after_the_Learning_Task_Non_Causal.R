###########################################
# Judgment Accuracy: Non-Causal Functions #
###########################################

#import libraries
library('lmerTest') 
library("tidyverse")
library('boot') # used for "inv.logit()" function

# Import data
Study2A_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal <- as_tibble(read.csv("Study2A_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal.csv"))

# Preview data
head(Study2A_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal)

# data dictionary:
## "Participant" = participant number
## "Function" = function number (non-causal functions only; range 5-6)
## "Strong_Preference" = denotes if participant had a strong preference for one of the policies (or not)
## "Judgment_Accuracy" = values are the distance a final judgment was from the correct assessment value (2 possible values: 0-1); 0=correct judgment; 1=incorrect (e.g., Policy A < Policy B)
## "Judgment_Bias" = values are distance from preferred policy judgment; 0=chose preferred policy as being best; 1=choose (correctly) that Policy A = Policy B; 2=choose non-preferred policy as being better

# Confidence Interval Function (used for computing confidence intervals from regression models)
CI_function <- function(model){
  m <- model
  se <- sqrt(diag(vcov(m)))
  tab <- cbind(Est = fixef(m), LL = fixef(m) - 2 * se, UL = fixef(m) + 2 * se)
  inv_tab <- inv.logit(tab) # probability scale
  labels <- c("Mean", "LowerBound", "UpperBound")
  values <- c(inv_tab[1], inv_tab[2], inv_tab[3])
  output <- data_frame(labels, values)
  output$labels <- as.factor(output$labels)
  return(output)
}

######################################
# Non-Causal: Judgment Bias Analysis #
######################################

jpe_non_causal_strong_preference <- Study2A_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal %>% 
  filter(Strong_Preference=="Yes") %>%  # omit neutral policies
  filter(Judgment_Bias!=1) # omit those who answered correctly

# Re-code so that 1 = choose preferred policy; 0 = choose non-preferred policy
jpe_non_causal_strong_preference$Judgment_Bias_Coded <-
  ifelse(jpe_non_causal_strong_preference$Judgment_Bias==0,1,0)


# Judgment Bias (only including people who did not correctly assess)
m1 <- glmer(Judgment_Bias_Coded ~ (1|Participant), 
            data=jpe_non_causal_strong_preference, family='binomial')
summary(m1)

# Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
# Family: binomial  ( logit )
# Formula: Judgment_Bias_Coded ~ (1 | Participant)
# Data: jpe_non_causal_strong_preference
# 
# AIC      BIC   logLik deviance df.resid 
# 77.7     82.2    -36.8     73.7       70 
# 
# Scaled residuals: 
#   Min     1Q Median     3Q    Max 
# -1.949  0.513  0.513  0.513  0.513 
# 
# Random effects:
#   Groups      Name        Variance  Std.Dev. 
# Participant (Intercept) 6.148e-15 7.841e-08
# Number of obs: 72, groups:  Participant, 62
# 
# Fixed effects:
#   Estimate Std. Error z value Pr(>|z|)    
# (Intercept)   1.3350     0.2902     4.6 4.22e-06 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

# get confidence interval from model
CI_function(m1)


#-----------------------------------------------------------------------------

######################################
# Strong vs Weak Preference Analysis #
######################################

# Effects coding for model
Study2A_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Strong_Preference2 <-
  ifelse(Study2A_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Strong_Preference=="No",.5,-.5)

# Re-code judgment accuracy so that 1=correct judgment; 0=incorrect judgment
Study2A_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Judgment_Accuracy_Coded <- 
  ifelse(Study2A_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Judgment_Accuracy==0,1,0)



# Descriptives
tapply(Study2A_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Judgment_Accuracy_Coded,
       list(Study2A_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal$Strong_Preference),
       mean)

# Modeling
m1 <- glmer(as.factor(Judgment_Accuracy_Coded) ~ Strong_Preference2 + (1+Strong_Preference2|Participant),
            data=Study2A_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal, family="binomial",
            control=glmerControl(optimizer="bobyqa"))
summary(m1)

# Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
# Family: binomial  ( logit )
# Formula: as.factor(Judgment_Accuracy_Coded) ~ Strong_Preference2 + (1 +      Strong_Preference2 | Participant)
# Data: Study2A_Judgments_of_Policy_Efficacy_after_the_Learning_Task_Non_Causal
# Control: glmerControl(optimizer = "bobyqa")
# 
# AIC      BIC   logLik deviance df.resid 
# 129.2    145.0    -59.6    119.2      171 
# 
# Scaled residuals: 
#   Min       1Q   Median       3Q      Max 
# -0.99006 -0.00721 -0.00452 -0.00448  1.02849 
# 
# Random effects:
#   Groups      Name               Variance Std.Dev. Corr
# Participant (Intercept)         388.6   19.71        
# Strong_Preference2 1296.8   36.01    0.51
# Number of obs: 176, groups:  Participant, 88
# 
# Fixed effects:
#   Estimate Std. Error z value Pr(>|z|)    
# (Intercept)        -10.3092     1.4074  -7.325 2.39e-13 ***
#   Strong_Preference2  -0.9307     2.8503  -0.327    0.744    
# ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# Correlation of Fixed Effects:
#   (Intr)
# Strng_Prfr2 -0.037

# get confidence interval from model
CI_function(m1)
